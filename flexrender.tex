
% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%




\documentclass[12pt]{ucthesis}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%    \pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue \fi

\usepackage{textcomp}
\usepackage{url}
\usepackage{listings}
\lstset{
	language=[Visual]C++,
	keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
	identifierstyle=\ttfamily,
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
	showstringspaces=false,
	basicstyle=\small,
	numberstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	numbersep=10pt,
	tabsize=2,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	breakatwhitespace=false,
	aboveskip={1.5\baselineskip},
  columns=fixed,
  upquote=true,
  extendedchars=true
% frame=single,
% backgroundcolor=\color{lbcolor},
}
\usepackage{color}
%\ifpdf

    \usepackage[pdftex]{graphicx}
    % Update title and author below...
    \usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                       linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                       bookmarksopenlevel=3,pdfstartview=FitV,
                                       pdfauthor=Christopher Gibson,
                                       pdftitle=Point-Based Color Bleeding With Volumes,
                                       pdfkeywords={thesis, masters, cal poly, volume rendering, global illumination}
                                       ]{hyperref}
    %Options with pdfstartview are FitV, FitB and FitH
    \pdfcompresslevel=1

%\else
%    \usepackage{graphicx}
%\fi

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}



%%%%%\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

\begin{document}

% Declarations for Front Matter

% Update fields below!
\title{FlexRender: A distributed rendering architecture for ray tracing huge
scenes on commodity hardware.}
\author{Robert Edward Somers}
\degreemonth{June} \degreeyear{2012} \degree{Master of Science}
\defensemonth{June} \defenseyear{2012}
\numberofmembers{3} \chair{Zo\"{e} Wood, Ph.D.} \othermemberA{Chris Lupo, Ph.D.} \othermemberB{Phillip Nico, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}



\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

As the quest for more realistic computer graphics marches steadily on, the
demand for rich and detailed imagery is greater than ever. Unfortunately, our
appetite for large and complex geometry is quickly outpacing advances in the
hardware used to render it. Scenes with hundreds of millions or even billions
of polygons are not only desired, they are demanded.

Techniques such as normal mapping and level of detail have attempted to address
the problem by reducing the amount of geometry in a scene. This is problematic
for applications that desire or demand access to the scene's full geometric
complexity at render time. More recently, out-of-core techniques have provided
methods for rendering large scenes when the working set is larger than the
available system memory.

We propose a distributed rendering architecture based on message-passing that
is designed to partition scene geometry across a cluster of commodity machines
in a spatially coherent way, allowing the entire scene to remain in-core and
enabling the construction of hierarchical spatial acceleration structures in
parallel. The results of our implementation show over an order of magnitude
speedup in rendering time compared the traditional approach, while keeping
memory overhead for message queuing around 1\%.

\textbf{TODO: Triple check that claim with the Toy Store scene results.}

\end{abstract}

%\begin{acknowledgements}

%   Thank you...

%\end{acknowledgements}


\tableofcontents


\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


% ------------- Main chapters here --------------------





\chapter{Introduction}
\label{intro}

\emph{Rendering}, the process of taking the description of a scene and turning
it into a visual image, has advanced at an incredible pace in recent
years. Many techniques have been developed to turn these descriptions into
pixels, and countless more have targeted weaknesses or challenges with those
algorithms. As the field continues to evolve and grow, new algorithms will be
born, others will fall out of favor and die, and some will soldier on. Only
one thing remains constant: The desire to bring our computational models ever
closer to mimicking physical reality.

\section{Geometric Complexity}
\label{complexity}

At the heart of mimicking reality is describing the world we wish to show. In
computer graphics, this has traditionally been done by defining surfaces. While
exciting developments in volume rendering techniques happen on regular basis,
it is unlikely we will abandon using surfaces any time soon. Unfortunately for
volumes, they are inherently an $n^3$ problem (where $n$ is the size of the
volume in one dimension) and $n^3$ is not a particularly friendly number in the
field of Computer Science.

For now and the foreseeable future, it seems, surfaces will be our bread and
butter. Many methods have been developed for describing surfaces to computer
programs, such as level sets, implicitly defined surfaces from mathematical
equations, and curvalinear forms such as parametric splines. The undisputed
champion of surface representations, however, has been the polygonal mesh.
Countless man-years of research and development has yielded efficient
techniques for processing meshes of interconnected polygons at blistering
speeds, and many other surface representations are ultimately converted to a
polygonal mesh at some stage of the rendering pipeline.

Meshes are easy for artists to work with because they represent discrete
points in space and the connectivity between those points (rather than abstract
equations). However, their core advantage is also their core drawback. Because
everything is defined explicitly, meshes with fine levels of detail have
significantly higher storage requirements. Thus, as the demand for higher
visual fidelity increases, the natural tendency is to increase geometric
complexity.

\section{Parallel Rendering}
\label{parallel}

Graphics has long been said to be a problem that is \emph{embarrasingly parallel},
given that many graphics algorithms operate on pixels independently. Graphics
processing units (GPUs) have exploited this fact for many years to achieve
amazing throughput of graphics primitives in real-time. Ray tracing in
particular (discussed more thoroughly in Section \ref{raytracing}) is commonly
paraded as the poster child for parallel processing.

Unfortunately the story is not all unicorns and rainbows. While processor
architectures have become exceedingly parallel and posted impressive
speedups, the memory hierarchy has not had time to catch up. For a processor
to perform well, the CPI, \emph{cycles per instruction}, must remain low to
ensure time is spent doing useful work and not waiting on data.

In current memory hierarchies, data access time can take anywhere from around
12 cycles (4 nanoseconds for an L1 cache hit) to over 300 cycles (100
nanoseconds for main memory). Techniques such as out-of-order execution are
helpful in filling this wasted time, but for memory intensive applications it
can be difficult to fill all the gaps with useful work. Thus, keeping the chips
"hot" by reducing time spent waiting on data is critical to achieve maximum
performance, and is an extremely challenging problem.

\textbf{TODO: graph of latencies in the memory hierarchy}

Because of this fact, there is a lot more to parallel rendering than initially
meets the eye. Graphics may indeed by highly parallel, but its voracious appetite
for memory access is actively working \emph{against} its parallel efficiency on
current architectures.

\section{Our Contribution}
\label{contribution}

This paper presents the architecture of FlexRender, a ray tracer designed for
rendering huge scenes with high geometric complexity on commodity hardware. We
specifically target commodity hardware because it currently has an excellent
cost to performance ratio, but still typically lacks enough memory to fit large
scenes entirely in RAM.

Current strategies for parallelizing a renderer across a cluster of commodity
machines are limited to having each worker compute a separate ``slice" of the
image, but do nothing to manage the high cost associated with large scene
assets.

Thus, our work describes the following core contributions:

\begin{enumerate}
    \item A system for ray tracing which uses the pooled memory of a cluster of
        commodity machines to keep the entire scene in-core.
    \item A method for passing ray messages between workers in the cluster with
        enough state to never require a reply message.
    \item An extension to the stackless BVH traversal algorithm presented by
        Hapala et al. \cite{hapala:2011} that makes it possible to suspend
        traversal at one worker and resume it at another.
    \item A discussion of the concepts involved and an analysis of the resulting
        implementation.
\end{enumerate}

In particular, we show that FlexRender can achieve speedups that exceed an
order of magnitude over the traditional parallelization approach, and can
naturally self-regulate the cluster of workers to keep the memory overhead due
to message queueing around 1\% of each worker's system memory.

\textbf{TODO: Triple check that claim with Toy Store results.}

\chapter{Background}
\label{background}

The FlexRender architecture builds on the four fundamenal building blocks
described here. First, we discuss the linearity of light (Section \ref{radiometry}),
which is critical to understanding why FlexRender produces a correct composite
image from pieces rendered by different workers. Next, we give a general
overview of ray tracing (Section \ref{raytracing}) and bounding volume hierarchies
(Section \ref{bvhs}). Finally, we discuss Morton coding and the Z-order curve
(Section \ref{morton}) which FlexRender uses to distribute scene data amongst the
workers.

\section{Light and Radiometry}
\label{radiometry}

\emph{Radiometry}, the study of propagation of electromagnetic radiation, forms
the basis for many rendering algorithms. Our coverage will be brief, but we will
examine some critical theory that enables the design of FlexRender. For a more
complete treatment of the topic in the context of rendering algorithms, we refer
the reader to \emph{Physically Based Rendering} \cite{pbrt}.

At its core, radiometry is based on modeling light as radiant energy and
operates at the level of geometric optics. In other words, we acknowledge that
light has wave-like properties and visible light occurs within a spectrum of
wavelengths (roughly 400 to 750 nm), but we do not mathematically model it as
a wave. Rather, we model it as a particle of radiant energy. In ray tracers
(discussed in depth in Section \ref{raytracing}) we model the path traveled by a single
particle of light with a ray.

The interesting parts of the radiometry model with respect to FlexRender are the
following observations:

\begin{description}
    \item[Light behaves linearly.] The combined effect of two rays of light
        in a scene is the same as the sum of their independent effects on the
        scene.
    \item[Energy is conserved.] When a ray reflects off of a surface, it can
        never do so with more energy than it started with.
\end{description}

In particular, these assumptions allow us to make the following key
observations, which FlexRender explicitly exploits:

\begin{description}
    \item[The location of computation does not matter.] If the scene is
        distributed across many workers, it makes no difference which worker
        computes the effect of a ray. The sum of all the workers' computations
        will be the same as if all the work was performed on a single worker.
    \item[Transmittance models energy conservation.] If we store the amount of
        energy traveling along a ray (the \emph{transmittance}) with the ray
        itself, we need not know anything about the preceeding rays or state
        that brought this ray into existence. We can compute its contribution
        to the scene independently and ensure that linearity and energy
        conservation are both respected.
\end{description}

\section{Ray Tracing}
\label{raytracing}

\textbf{TODO: general overview (mention Monte Carlo)}

\section{Bounding Volume Hierarchies}
\label{bvhs}

Bounding volume hierarchies, or BVHs, are essentially an application of binary
search to 3D space. In ray tracing, the vast majority of time is spent
computing intersection tests to determine whether a ray intersects a given
primitive. BVHs allow us to search 3D space for a potential intersection in a
binary tree fashion, pruning off large numbers of primitives with a single
test. Since it is effectively binary search, it is unsurprising that it reduces
intersection searching from linear time $O(n)$ to logarithmic time $O(log\;n)$
(assuming the BVH is well formed).

\textbf{TODO: picture of bvh}

BVHs are trees where each node is defined by a bounding volume, such as a box
or a sphere, that describes the extents of a region of 3D space. All of the
primitives in the scene that are within that region of space are child nodes
in the tree. Each node has two direct children, which also define bounding
volumes within that subregion of space. Nodes which have other bounding volume
nodes as their children are \emph{interior} nodes. \emph{Leaf} nodes in the tree
define a bounding volume around a single primitive.

After the tree structure is built using a typical linked-memory data structure,
it is flattened into a linear array of nodes for storage efficiency and better
cache performance. Flattening is done by walking the tree in a depth-first
traversal. This means that the left-hand child of the node at index $n$ is
located at index $n + 1$ in the array of nodes. Each node stores an offset
index to its right-hand child. Because the indices are relative to the array
and not the array's location in memory, they are safe to pass back and forth
between workers and require no address translation.

To traverse the tree looking for intersections, we begin by testing the root
node's bounding volume for an intersection. If the ray intersects the volume,
we recursively test each of the node's children. Once we hit a leaf node,
we perform an actual ray-primitive intersection test to determine if the ray
hits the primitive.

The traversal algorithm is naturally recursive, but recursive implementations
keep their state on the call stack. In FlexRender, we may need to suspend the
traversal on one worker and resume it on another, so we need all of the
traversal state explicitly exposed. Refactoring it as an iterative traversal
explicitly exposes the state for capturing.

The iterative algorithm still requires a traversal stack of child nodes that
need to be visited on the way back up the tree. Since this stack is unique for
each ray, the entire stack would need to be carried along with each ray. In
addition, the stack could potentially be large if the scene is huge (and thus,
the tree is deep), so it would be ideal if we had a method for traversing the
tree iteratively without the need to maintain a traversal stack.

A method for doing so is described by Hapala et al. in
\emph{Efficient Stack-less BVH Traversal for Ray Tracing} \cite{hapala:2011}.
Their key insight is that if parent links are stored in the tree, the same
traversal can be achieved using a three-state automaton describing the direction
you came from when you reached the current node (i.e. from the \emph{parent},
from the \emph{sibling}, or from the \emph{child}). They show that their
traversal algorithm produces identical tree walks and never retests nodes that
have already been tested.

FlexRender leverages this traversal algorithm due it its low state storage
requirements. Each ray only needs to know the index of the current node it is
traversing (in the array of flattened nodes) and the state of the traversal
automaton. The extensions made to the algorithm to support suspending traversal
and resuming it on another worker are described in Section \ref{traversal}.

\section{Morton Coding and the Z-Order Curve}
\label{morton}

Morton coding is a method for mapping coordinates in multidimensional space to
a single dimension. In particular, walking the multidimensional space with a
Morton coding produces a space-filling Z-order curve.

\textbf{TODO: pictures of Z-order curves}

More concretely, FlexRender needs a way to distribute a large scene to many
workers in a spatially coherent way. If the geometry on each worker consists
of a localized patch of the overall geometry, it allows us minimize communication
between the workers, and thus, only pay the network cost when we absolutely need
to (described in depth in Section \ref{traversal}).

Because the Morton coding produces a spatially coherent traversal of 3D space,
dividing up the 1D Morton code address space among all the workers participating
in the render gives a reasonable assurance of spatial locality for the geometry
sent to each worker.

The Morton coding is relatively simple to implement. For example, say that
we wish to map a point $P$ in a region of 3D space (defined by its bounding extents
$min$ and $max$) to a Morton coded 64-bit integer. Discretizing each axis evenly
allows for 21 bits per axis, yielding a 63-bit address space (and one unused bit
in the integer).

Computing the Morton code is simply a matter of calculating the 21-bit discretized
component of $P$ along each axis, then shifting the components from each axis
into the 64-bit integer one bit at a time in a round robin fashion, from the
most-significant to least-significant bit.

\textbf{TODO: pseudocode listing of morton code computation}

\chapter{Related Work}
\label{relatedwork}

\textbf{TODO}

\section{Big Geometry Workarounds}
\label{managingcomplexity}

\textbf{TODO}

\subsection{Normal Mapping}
\label{normalmaps}

\textbf{TODO: seminal work}

\subsection{Level of Detail}
\label{levelofdetail}

\textbf{TODO: seminal work}

\subsection{Out-of-Core}
\label{outofcore}

\textbf{TODO: dreamworks PBGI paper, panta ray for directional occlusion}

\section{Parallel Rendering}
\label{parallelbg}

\textbf{TODO}

\subsection{Cooperative Networked Rendering}
\label{networked}

\textbf{TODO: how we differ from kilauea (refer to subsections) \cite{kato:2002}}

\subsection{General Purpose GPU Rendering}
\label{gpgpu}

\textbf{TODO: ray batching, parallel BVH construction, branchy/memory drawbacks (shaders)}

\chapter{FlexRender Architecture}
\label{architecture}

In this chapter, we describe the network architecture and roles of the involved
machines in Section \ref{workers}. We discuss the structure of ray messages
passed between workers in Section \ref{fatrays} and briefly cover the design of
the graphics machinery shared between FlexRender and the baseline implementation
in Section \ref{types}. We detail the process of preparing the cluster for
rendering in Section \ref{sync} and Section \ref{parallelbvh}.

At its core, workers in the cluster are just ray processors. We discuss how they
manage ray messages in Section \ref{queues}, how they generate new work with
stable memory usage in Section \ref{primaryrays}, how we decide when and where
to send rays over the network in Section \ref{traversal}, and how shading is computed
when lights and occluders may exist anywhere in the cluster in Section \ref{shading}.

Finally, we wrap up our discussion with how we monitor the progress of the
render in Section \ref{stats} and how we composite the final image from its
components in Section \ref{synthesis}.

\section{Organization and Design}
\label{organization}

\subsection{Workers and the Renderer}
\label{workers}

There are two potential roles a machine can play during the rendering process.

\begin{description}
    \item[Worker] These machines receive a chunk of the scene and act as ray
        processors to compute intersections and shading values. They produce
        an image that is a component of the final render. There may be an
        arbitrary number of them participating in a render.
    \item[Renderer] This machine reads in the scene data and distributes it to
        the workers. Once rendering begins it monitors the status of the each
        worker and halts any potential runaway situations (see Section \ref{primaryrays}).
        When the renderer decides the process is complete, it requests
        the image components from each worker and merges them into the final
        image. There is only a single renderer in any given cluster and it is
        the machine the user directly interacts with.
\end{description}

From a network perspective, the architecture is simply client/server connected
in a star configuration. Each worker exposes a server which receives and
processes messages, and holds client connections open to every other worker for
passing messages around the cluster. The renderer also holds a client connection
to every worker for sending configuration data, preparing up the cluster for rendering
(described in Section \ref{sync}), and monitoring render progress (described in
Section \ref{stats}).

\textbf{TODO: diagram of network architecture}

\subsection{Fat Rays}
\label{fatrays}

The currency of computation and core message type in FlexRender is the
\emph{fat ray}. They are so named because they carry additional state
information along with their geometric definition of an origin and a direction.
Their counterparts, \emph{slim rays}, consist of only the geometric components.

Specifically, a fat ray contains the following data:

\begin{itemize}
    \item The \textbf{type of ray} this is. Described in Section \ref{process}.
    \item The \textbf{source pixel} that this ray contributes to.
    \item The \textbf{bounce count}, or number of times this ray has reflected
        off a surface (to prevent infinite loops).
    \item The \textbf{origin and direction} of the ray.
    \item The ray's \textbf{transmittance}, or the amount that it contributes
        to the source pixel.
    \item The \textbf{emission} from a light source carried along the ray (if
        any). Described in Section \ref{shading}.
    \item The \textbf{target intersection point} of the ray, if any. Described
        in Section \ref{shading}.
    \item The \textbf{traversal state} of the top-level BVH. Described in
        Section \ref{traversal}.
    \item The \textbf{hit record}, which contains the worker, mesh, and $t$
        value of the nearest intersection.
    \item The \textbf{current worker} this ray should be sent to over the
        network.
    \item The \textbf{number of workers touched} by the ray so far. Not
        necessary for rendering. Only used for analysis.
    \item A \textbf{next pointer} for locally queueing rays as described in
        Section \ref{queues}. Obviously not valid over the network.
\end{itemize}

In total, the size of a fat ray is 128 bytes.

\subsection{General Types}
\label{types}

The core graphics machinery in FlexRender is fairly straightforward. A scene consists
of a collection of \emph{meshes}, which are stored as indexed face sets of vertices
(positions with normals and texture coordinates) and faces.

Each mesh is a assigned a \emph{material}, which is responsible for drawing the
mesh. A material consists of a \emph{shader} and potentially a set of bindings
from \emph{textures} to names in the shader.

A \emph{shader} is a piece of code that is run to compute the lighting on a
surface at a particular point.

A \emph{texture} is a either a 2D array of pixels (for image textures) or
a snippet of code (for procedural textures) that defines the value of
something across a surface. They are most commonly used for providing colored
detail across the face of a triangle.

All of the code used in graphics computation is shared between both our baseline
implementation and FlexRender. This ensures fair comparisons when we analyze our
results in Section \ref{results}.

\section{Render Preparation}
\label{prep}

Render preparation consists of ensuring that all the workers agree on the
basic rendering parameters, distributing the scene assets to each worker, and
building the distributed bounding volume hierarchy. We discuss configuration
and distribution in Section \ref{sync} and parallel construction of the BVH in
Section \ref{parallelbvh}.

\subsection{Configuration and Asset Distribution}
\label{sync}

The configuration consists of basic information about how the scene is to be
rendered, such as the image dimensions, the workers participating in the render,
etc. In addition, the configuration specifies the minimum and maximum bounding
extents of the scene. This is used in the asset distribution step for driving
the Morton coding discretation along each axis.

Once the renderer has read in the configuration, it opens client connections
to each worker and sends the configuration data to them. Similarly, each worker
opens a client connection to every other worker. These renderer/worker and
worker/worker connections remain open for the duration of the render.

The renderer then divides up the 63-bit Morton code address space evenly by the
number of workers participating and assigns a region of it to each one.

Finally, the renderer begins reading in and parsing the scene data. As each mesh
is loaded, the renderer takes the following actions:

\begin{enumerate}
   \item Computes the centroid of the mesh by averaging its vertices.
   \item Computes the Morton code of the mesh centroid. This determines
      which worker the mesh will be sent to.
   \item Ensures that any asset dependencies (such as materials,
      shaders, and textures) for this mesh have been sent the designated worker.
   \item Sends the mesh data to the designated worker.
   \item Deletes the mesh data from its own memory.
\end{enumerate}

Although the current implementation reads scene data in at the renderer
and distributes it over the network, there is no inherent reason why it needs to
do so. For example, if all the workers have access to the scene data over network
storage, the renderer could simply tell each worker the range of the Morton code
address space it is responsible for and let them load the scene data themselves,
carefully throwing out any geometry that is not within their Morton bounds.

\subsection{Parallel Construction of Spatial Acceleration Structures}
\label{parallelbvh}

To accelerate intersection testing against each mesh, each worker first
constructs a BVH for every mesh it has. These BVHs are tied directly to the mesh
they accelerate. While building each BVH, the worker stores a list of the bounding
extents of each mesh.

Next, each worker builds a root BVH for the entire worker, which uses the mesh
extents for its leaf nodes. When testing for intersections locally, a worker
first tests against this root BVH to determine candidate meshes, then traverses
each mesh's individual BVH to compute absolute intersections. After construction
of this root BVH, the root node's bounding extents will describe the spatial
extent of all geometry located on that worker.

Once construction of all the local BVHs is complete, each worker sends its
total bounding extents to the renderer. Once the renderer has the bounding
extents of each worker, it constructs a final ``top-level" BVH of the workers.
The renderer then distributes this top-level BVH to all the workers, so that
everyone participating has an identical copy of it. This is a very quick and
lightweight process, since it is only dependent on the number of workers
participating in the render.

This top-level BVH will be used to direct where we pass rays over the network
in Section \ref{traversal}.

\section{Ray Processing}
\label{process}

At their core, workers are essentially just multithreaded ray processors. Once
rendering begins, they continually pull rays out of the ray queue (discussed in
Section \ref{queues}), schedule them onto the thread pool, and process them
when the thread is run. This processing step consists of testing for intersection,
potentially forwarding the ray to another worker (Section \ref{traversal}),
or computing shading values if the ray terminates (Section \ref{shading}).

As mentioned in Section \ref{fatrays}, fat rays have an associated type. There
are three different types of rays in FlexRender:

\begin{itemize}
   \item \textbf{Intersection rays} are rays whose sole purpose is to identify
      a point in space at which we would like to compute shading.
   \item \textbf{Illumination rays} are essentially copies of intersection
      rays that have terminated. They are sent to workers who have emissive
      geometry (described in Section \ref{shading}).
   \item \textbf{Light rays} are Monte Carlo samples that contribute direct
      illumination to a point that a worker is shading (described in Section
      \ref{shading}).
\end{itemize}

An important point to note about these rays is the sequence of their lifetimes.
Their purposes will become more clear in the following sections, but the order
is as follows:

\begin{enumerate}
   \item An intersection ray is cast into the scene.
   \item When that intersection ray terminates at a point in space, it dies and
      spawns illumination rays.
   \item When those illumination rays reach their destination, they die and
      spawn light rays.
   \item When those light rays terminate at a point in space, a shading value
      is computed.
\end{enumerate}

From this sequence of lifetimes, it should be apparent that a single intersection
ray can spawn many additional rays. It should also be apparent that light rays
are the most likely to die without generating more rays.

\subsection {Ray Queues}
\label{queues}

Each worker has ray queue, with the typical push and pop operations for adding
and remove rays from the queue. This queue internally is implemented as three
separate queues, where rays are separated by type. It also contains information
about the scene camera, for generating new primary rays.

When a new ray arrives at a worker over the network, it is immediately pushed
into the queue. Internally, it is pushed into the queue matching its type
(intersection, illumination, or light).

When the worker pops a ray from the queue, we pull rays from the internal
queues in the following order:

\begin{enumerate}
   \item The \textbf{light queue}, since these are least likely to generate
      new rays.
   \item If the light queue is empty, we pop from the \textbf{illumination queue},
      since these will generate a limited number of new rays.
   \item If the illumination queue is empty, we pop from the
      \textbf{intersection queue}, since these can generate the most new rays.
   \item If all of the internal queues are empty, we use the camera to cast
      new primary rays into the scene (described in \ref{primaryrays}). This
      effectively generates new work.
\end{enumerate}

Organizing the processing order of rays as a priority queue based on ray type
is an essential step in minimizing the exponential explosion of work than can
occur if too many primary rays enter the system at a time. This helps reduce
memory usage required for queueing messages because new work is not generated
until the system is ready to handle it.

\subsection{Primary Ray Casting}
\label{primaryrays}

The workers in the cluster are each responsible for casting a portion of the
primary rays in the scene. The reasoning behind this is to give the cluster
the ability to regulate itself.

Consider the case where a worker has not received any work recently from other
workers in the cluster. For whatever reason (intersection tests, shading, etc.)
the other workers are too busy dealing with their own queues to send any work
in the direction of our lonely, rayless worker. By giving the worker control
over primary ray casting for a portion of the scene, we give workers the ability
to generate work when they have nothing to do.

However, to quote a comic book character, ``with great power comes great
responsibility". Consider the case where a worker contains mostly background
geometry. It will be receiving work from others infrequently because its size
in screen space is small, but yet it is in charge of casting primary rays for
a much larger slice of the image. In this case the worker may go on an
unfettered spree of primary ray casting, causing lots of grief for his fellow
workers since he is essentially generating work for others and little for
himself.

To prevent this ``runaway" case from overburdening the ray queues, we require
that workers report statistics about their progress to the renderer every so
often (10 times per second in the current implementation). If the renderer
notices that a worker is getting significantly further ahead of the others in
primary ray casting, we temporarily disable primary ray casting on that worker
until the others catch up.

Because the other workers will not generate primary rays as long as they still
have other rays in their queues to process, the priority queueing, shared
ray casting responsibilities, and temporary pausing of primary ray generation at
runaway workers provides a simple means of self-regulating the cluster that
works remarkably well and minimizing the memory overhead for message queueing.
We will examine the overhead in detail in Chapter \ref{results}.

\subsection{Distributed BVH Traversal}
\label{traversal}

\textbf{TODO: modifications to stackless method}

\subsection{Illumination and Shading}
\label{shading}

There is no special ``light" type in FlexRender, rather only meshes that are
emissive, which is a property set by the assigned material. Meshes that are
emissive are known to inject light into the scene through a special function in
their shader which is called to compute that emission. During scene loading,
the renderer maintains a list of all workers that have received at least one
emissive mesh. This list is synchronized with all workers right before
rendering begins.

Once a point has been indentified for shading with intersection testing, we
need to determine the visibility of that point with respect to the light sources
in the scene. In a traditional ray tracer, you simply cast rays from the point
of intersection to samples on the surface of the light to determine visibility.
However in FlexRender, from the perspective of the worker doing the shading,
we have no idea where all the of the lights in the scene are, or more importantly,
if there is any geometry occluding the light and casting the intersection in
shadow.

To solve this problem, we simply trace the light rays in the opposite direction.
Rather than originating from the intersection point and heading in the
direction of the light, we originate the rays at the light and cast them in the
direction of the intersection point. If the light ray arrives back at the
original intersection point after being traced through the cluster, we know that
its path from the light was unoccluded. This leverages the same efficient method
for distributed ray tracing that we already developed for testing intersections.

Specifically, casting light rays towards an intersection point consists of the
following operations:

\begin{enumerate}
    \item At the worker where an intersection was found, copy the ray data into
        an illumination ray with the target set to the point of intersection.
    \item Send this illumination ray to all workers known to have emissive meshes.
    \item When a worker receives an illumination ray, generate sample points
        across the surface of all emissive meshes. Set these sample points as
        the origins for new light rays.
    \item Set the directions of all the light rays such that they are pointed
        in the direction of the target (the original point of intersection).
    \item Push each light ray into the ray queue and let the cluster process
        them as usual.
\end{enumerate}

When a light ray arrives at a worker, we simply check that its point of
intersection is within some epsilon of the target. If it is, we consider the
light sample visible, look up the material, shader, and textures for the mesh,
and run the shader. The shader is responsible for writing its computed values
into the worker's image buffer.

The implementation of shaders in FlexRender is through extensions to the Lua
\cite{lua} programming language, using the LuaJIT \cite{luajit} implementation
for speed. A shader may do any (or none) of the following:

\begin{enumerate}
    \item Sample textures based on the name bindings assigned in the material
        definition.
    \item Compute a light value based on some implementation of a mathematical
        shading model (such as the Phong model) and local information at the
        point being shaded (such as the interpolated surface normal and texture
        coordinates).
    \item Accumulate computed light values into the primary RGB buffers, or
        any auxilliary named buffer.
    \item Cast additional rays into the scene.
\end{enumerate}

\textbf{TODO: example phong shader snippet}

When new rays are cast into the scene from a shader, the results of that trace
are not immediately available. Instead, the trace pushes the new rays into the
queue for processing and the traversal and shading systems ensure that the
result of secondary and $n$-ary traced rays will be included in the final image.

In order for linearity and energy conservation to be respected, certain values
are inherited from the parent ray. In particular, the source pixel is inherited
(so the ray contributes to the correct pixel in the final image) and the
desired transmittance along the new ray is multiplied with the transmittance of
the parent ray (to ensure energy conservation is preserved).

Casting rays from the shader can be used to implement several common visual
effects:

\begin{itemize}
    \item \textbf{Alpha masking} can be achieved by casting a new ray in the
        same direction with the full transmittance of the parent ray.
    \item \textbf{Reflection} can be achieved by casting a new ray in the
        reflected direction with some fraction of the transmittance based on
        the reflectivity of the surface.
    \item \textbf{Refraction} can be achieved by casting a new ray in the
        refracted direction with some fraction of the transmittance based on
        the index of refraction and opacity of the surface.
    \item \textbf{Monte Carlo global illumination} can be achieved by casting
        rays in random directions by sampling the hemisphere above the surface,
        with fractional transmittances based on the number of samples.
\end{itemize}

\section{Render Completion}
\label{completion}

In a traditional recursive ray tracer, determining when the render is complete
is a simple task. Once the last primary ray pops its final stack frame frame
the render is over. In FlexRender, however, no one worker (or the renderer, for
that matter) knows where the ``last ray" is. To determine when a render is
complete, the workers report rendering statistics to the renderer at regular
intervals, which are used for deciding when the render is finished. This decision
metric is discussed in Section \ref{stats}.

Once the render is complete, the renderer requests the individual image buffers
from each worker and reassembles them into the final output image. This process
is discussed in Section \ref{synthesis}.

\subsection{Statistics and Monitoring}
\label{stats}

For monitoring the progress of the render, the workers report general statistics
about their activities to the renderer at regular intervals (in our current
implementation, 10 times per second). In particular, the following four
statistics are useful for determining if the render has finished:

\begin{description}
   \item[Primary Ray Casting Progress] The amount of the worker's primary rays
      that have been cast into the scene.
   \item[Rays Produced] The number of rays created at the worker during this
      measurement interval. This includes new intersection rays cast from the
      camera or a shader, illumination rays created by terminating intersection
      rays, or light rays created by processing illumination rays.
   \item[Rays Killed] The number of rays finalized at the worker during this
      measurement interval. This includes intersection rays that terminated or
      did not hit anything, illumination rays that were destroyed after spawning
      light rays, or light rays whose final shading value was computed.
   \item[Rays Queued] The number of rays currently in this worker's ray queue.
\end{description}

In particular, if any worker has not finished casting its primary rays, we
know for certain the render is not complete. Secondly, if we observe that no
rays are produced, killed, or queued at any workers for some number of
consecutive measurement intervals, it is reasonable to assume that the render
has concluded.

Our current implementation (which reports statistics 10 times per second)
waits until it sees 10 consecutive intervals of ``uninteresting" activity on
the workers before declaring the render complete. We find this achieves a nice
balance between wanting to end the render as soon as it is legitimately
finished and risking concluding it too early.

\subsection{Image Synthesis}
\label{synthesis}

Once the render has been deemed ``complete", the renderer requests the image
buffers from each worker. Because all rendering was computed by respecting
linearity, computing the a pixel in the final output image is just a matter
of summing corresponding pixels in the worker buffers.

This can yield some interesting intermediate images. Each worker's buffer
represents the light in the final image that interacts in some way with the
geometry present on that worker.

For direct light, this shows up as shaded samples where geometry was present
and black areas where it was not. For other effects such as reflections and
global illumination, it appears that the worker had access to the whole scene's
geometry all along, but this is just an optical illusion. Since the rays
carry with them the source pixel they contribute to (and this value is inherited
as new rays are spawned from the primary ray), a worker can end up contributing
to any pixel in the image as long as the light interacted in some way with the
geometry it controlled.

\textbf{TODO: show examples of intermediate images}

\chapter{Results}
\label{results}

\textbf{TODO}

\chapter{Future Work}
\label{futurework}

Given that FlexRender is a relatively radical reimagining of ray tracer
architecture, the opportunities for improvement are vast. In this section we
document larger issues we noticed during implementation and testing that
could potentially offer significantly better results.

\section{System Optimizations}
\label{optimizations}

We present three potential optimizations. Two would deter or eliminate the
extra network hops in BVH traversal, and the third would provide better
geometry distribution and more parallelism.

The first extra network hop occurs when rays are generated on machines that
control geometry they do not intersect with. This requires at least one
network hop before the ray could potentially intersect and be shaded. By performing
a prepass step before rendering began, we could gather information about the
layout of the scene in image space. By carefully choosing the slices of the
image that workers were responsible for casting primary rays in, we could
dramatically increase the chances that the ray would intersect the geometry
on the worker and finish shading without ever touching the network.

The second extra network hop occurs at the end of traversal, when the ray finishes
traversing the top-level BVH but ends up on a worker that is not the point of
intersection. Because the worker has no shading assets for that geometry, it
must pass the ray back to the ``winning" worker for shading. By distributing
all of the shading assets (shaders, textures, and materials) to all workers,
any worker would be capable of shading a ray at the conclusion of its BVH traversal.
This is possible because the rays carry along local geometry (mainly the surface
normal) from the point of closest intersection.

Lastly, we believe, in retrospect, that the Morton coding and Z-order curve was
a poor choice for the distribution of geometry. It was initially chosen because
we did not want to require a preprocessing (or \emph{baking}) step for
determining the optimal distribution of geometry. Since the scene was presumably
too large to fit in core on any one machine, preprocessing the scene as a whole
was deemed to be against the spirit of the problem.

The Z-order curve works well for spatially uniform scenes, but unfortunately a
lot of scenes that are interesting to render are usually uniformly distributed
in image space, not 3D space. The leads to a lot of degenerate scenes that
perform suboptimally when rendered with FlexRender.

There are lots of opportunities for distributing geometry in a more intelligent
way if a preprocessing step was allowed. In addition, distribution could be
designed for not only memory availability, but also for estimated rendering
workload. This would lead to better parallelism during rendering, a more
even computational load across the cluster, and better render times.

\section{Memory Optimizations}
\label{memory}

Based on the profiling results of our current implementation, the main
bottleneck is standard library calls and memory allocation. The transient
nature of many fixed-size fat rays is a perfect opportunity to use an object
pool to reduce allocation overhead and heap fragmentation.

In addition, the linear BVH nodes structures were intentionally padded to
64-bytes to match the cache line size on current CPUs. However without a custom
STL allocator for the vector that stores this array of nodes, cache alignment
is not guaranteed.

Lastly, there are a few places where the size of fat rays could be reduced.
For example, rather than storing the target position of an expected
intersection, the expected t value along the ray could be stored instead. This
would save 8 bytes. Another 4 bytes could be saved by removing the ``workers
touched" benchmarking field. By careful overlapping of mutually exclusive data
using unions, the size could probably be reduced further. Ideally a fat ray
would fit on a single cache line and the same aligned allocation strategy could
be applied to fat rays as well.

\section{GPGPU and Heterogenous Architectures}
\label{hetergenous}

One of FlexRender's strengths is that it decouples the ray tracing computations
at the level of the individual ray. Because workers in the cluster speak a
defined network protocol to pass rays around, this opens the door for workers to
join the cluster with new and interesting machine architectures.

For example, consider general purpose GPU (GPGPU) computing. Vast speedups can
be attained by moving number-crunching operations to the GPU, but the CPU and
GPU usually exist in separate memory spaces. This means the memory transfer cost
of pushing data over the bus to the GPU must be amortized by the amount of
computation done on the GPU.

Because workers are never waiting on other workers for the results of a
computation, this opens up the possibility for batching data to a GPU worker.
Consider a worker who simply queues messages until enough are present to move
the data to the the GPU. FlexRender's architecture is amiable to amortizing
the cost of bus transfers in GPGPU computing.

\clearpage
\bibliography{flexrender}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\section*{Image Results}

\end{document} much faster 
