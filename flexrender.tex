
% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%




\documentclass[12pt]{ucthesis}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%    \pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue \fi

\usepackage{textcomp}
\usepackage{url}
\usepackage{listings}
\lstset{
	language=[Visual]C++,
	keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
	identifierstyle=\ttfamily,
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
	showstringspaces=false,
	basicstyle=\small,
	numberstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	numbersep=10pt,
	tabsize=2,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	breakatwhitespace=false,
	aboveskip={1.5\baselineskip},
  columns=fixed,
  upquote=true,
  extendedchars=true
% frame=single,
% backgroundcolor=\color{lbcolor},
}
\usepackage{color}
%\ifpdf

    \usepackage[pdftex]{graphicx}
    % Update title and author below...
    \usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                       linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                       bookmarksopenlevel=3,pdfstartview=FitV,
                                       pdfauthor=Christopher Gibson,
                                       pdftitle=Point-Based Color Bleeding With Volumes,
                                       pdfkeywords={thesis, masters, cal poly, volume rendering, global illumination}
                                       ]{hyperref}
    %Options with pdfstartview are FitV, FitB and FitH
    \pdfcompresslevel=1

%\else
%    \usepackage{graphicx}
%\fi

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}



%%%%%\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

\begin{document}

% Declarations for Front Matter

% Update fields below!
\title{FlexRender: A distributed rendering architecture for ray tracing huge
scenes on commodity hardware.}
\author{Robert Edward Somers}
\degreemonth{June} \degreeyear{2012} \degree{Master of Science}
\defensemonth{June} \defenseyear{2012}
\numberofmembers{3} \chair{Zo\"{e} Wood, Ph.D.} \othermemberA{Chris Lupo, Ph.D.} \othermemberB{Phillip Nico, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}



\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

As the quest for more realistic computer graphics marches steadily on, the
demand for rich and detailed imagery is greater than ever. Unfortunately, our
appetite for large and complex geometry is quickly outpacing advances in the
hardware used to render it. Scenes with hundreds of millions or even billions
of polygons are not only desired, they are demanded.

Techniques such as normal mapping and level of detail have attempted to address
the problem by reducing the amount of geometry in a scene. This is problematic
for applications that desire or demand access to the scene's full geometric
complexity at render time. More recently, out-of-core techniques have provided
methods for rendering large scenes when the working set is larger than the
available system memory.

We propose a distributed rendering architecture based on message-passing that
is designed to partition scene geometry across a cluster of commodity machines
in a spatially coherent way, allowing the entire scene to remain in-core and
enabling the construction of hierarchical spatial acceleration structures in
parallel. The results of our implementation show over an order of magnitude
speedup in rendering time compared the traditional approach, while keeping
memory overhead for message queuing around 1\%.

\textbf{TODO: Triple check that claim with the Toy Store scene results.}

\end{abstract}

%\begin{acknowledgements}

%   Thank you...

%\end{acknowledgements}


\tableofcontents


\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


% ------------- Main chapters here --------------------





\chapter{Introduction}
\label{intro}

\emph{Rendering}, the process of taking the description of a scene and turning
it into a visual image, has advanced at an incredible pace in recent
years. Many techniques have been developed to turn these descriptions into
pixels, and countless more have targeted weaknesses or challenges with those
algorithms. As the field continues to evolve and grow, new algorithms will be
born, others will fall out of favor and die, and some will soldier on. Only
one thing remains constant: The desire to bring our computational models ever
closer to mimicking physical reality.

\section{Geometric Complexity}
\label{complexity}

At the heart of mimicking reality is describing the world we wish to show. In
computer graphics, this has traditionally been done by defining surfaces. While
exciting developments in volume rendering techniques happen on regular basis,
it's unlikely we'll abandon using surfaces any time soon. Unfortunately for
volumes, they are inherently an $n^3$ problem and $n^3$ is not a particularly
friendly number in the field of Computer Science.

For now and the foreseeable future, it seems, surfaces will be our bread and
butter. Many methods have been developed for describing surfaces to computer
programs, such as level sets, implicitly defined surfaces from mathematical
equations, and curvalinear forms such as parametric splines. The undisputed
champion of surface representations, however, has been the polygonal mesh.
Countless man-years of research and development has yielded efficient
techniques for processing meshes of interconnected polygons at blistering
speeds, and many other surface representations are ultimately converted to a
polygonal mesh at some stage of the rendering pipeline.

Meshes are easy for artists to work with because they represent discrete
points in space and the connectivity between those points (rather than abstract
equations). However, their core advantage is also their core drawback. Because
everything is defined explicitly, meshes with fine levels of detail have
significantly higher storage requirements. Thus, as the demand for higher
visual fidelity increases, the natural tendency is to increase geometric
complexity.

\section{Parallel Rendering}
\label{parallel}

Graphics has long been said to be a problem that is \emph{embarrasingly parallel},
given that many graphics algorithms operate on pixels independently. Graphics
processing units (GPUs) have exploited this fact for many years to achieve
amazing throughput of graphics primitives in real-time. Ray tracing in
particular (discussed more thoroughly in \ref{raytracing}) is commonly
paraded as the poster child for parallel processing.

Unfortunately the story isn't all unicorns and rainbows. While processor
architectures have become exceedingly parallel and posted impressive
improvements, the memory hierarchy hasn't had time to catch up. In fact,
if anything the problem has gotten more muddled and complicated as architects
have scrambled to dump layer upon layer of cache in front of the memory
controller to try to hide the fact that keeping these chips hot all the time is
actually an extremely challenging problem.

Because of this, there is a lot more to parallel rendering than initially meets
the eye. Graphics may indeed by highly parallel, but it is also hugely memory
intensive, and that fact is actively working \emph{against} its parallel
nature on current architectures.

\section{Our Contribution}
\label{contribution}

This paper presents the architecture of FlexRender, a ray tracer designed for
rendering huge scenes with high geometric complexity on commodity hardware. We
specifically target commodity hardware because it currently has an excellent
cost to performance ratio, but still typically lacks enough memory to fit large
scenes entirely in RAM.

Current strategies for parallelizing a render across a cluster of commodity
machines are limited to having each worker compute a separate ``slice" of the
image, but do nothing to manage the high cost associated with large scene
assets.

Thus, our work describes the following core contributions:

\begin{enumerate}
    \item A system for ray tracing which uses the pooled memory of a cluster of
        commodity machines to keep the entire scene in-core.
    \item A method for passing ray messages between workers in the cluster with
        enough state to never require a response.
    \item An extension to the stackless BVH traversal algorithm presented by
        Hapala et al. \cite{hapala:2011} that makes it possible to suspend
        traversal at one worker and resume it at another.
    \item A discussion of the concepts involved and an analysis of the resulting
        implementation.
\end{enumerate}

In particular, we show that FlexRender can achieve speedups that exceed an
order of magnitude over the traditional parallelization approach, and can
naturally self-regulate the cluster of workers to keep the memory overhead due
to message queueing around 1\% of each worker's system memory.

\textbf{TODO: Triple check that claim with Toy Store results.}

\chapter{Background}
\label{background}

\textbf{TODO}

\section{Light and Radiometry}
\label{radiometry}

\textbf{TODO: light is linear}

\section{Ray Tracing}
\label{raytracing}

\textbf{TODO: general overview}

\section{Bounding Volume Hierarchies}
\label{bvhs}

\textbf{TODO: basic structure and traversal, algorithmic performance, etc.}

\section{Morton Coding and the Z-Order Curve}
\label{mortoncoding}

\textbf{TODO: spatially coherent mapping of 3D space to 1D space}

\chapter{Related Work}
\label{relatedwork}

\textbf{TODO}

\section{Big Geometry Workarounds}
\label{managingcomplexity}

\textbf{TODO}

\subsection{Normal Mapping}
\label{normalmaps}

\textbf{TODO: seminal work}

\subsection{Level of Detail}
\label{levelofdetail}

\textbf{TODO: seminal work}

\subsection{Out-of-Core}
\label{outofcore}

\textbf{TODO: dreamworks PBGI paper, panta ray for directional occlusion}

\section{Parallel Rendering}
\label{parallelbg}

\textbf{TODO}

\subsection{Cooperative Networked Rendering}
\label{networked}

\textbf{TODO: how we differ from kilauea (refer to subsections) \cite{kato:2002}}

\subsection{General Purpose GPU Rendering}
\label{gpgpu}

\textbf{TODO: ray batching, parallel BVH construction, branchy/memory drawbacks (shaders)}

\chapter{FlexRender Architecture}
\label{architecture}

\textbf{TODO}

\section{Organization and Design}
\label{organization}

\textbf{TODO}

\subsection{General Types and Subsystems}
\label{types}

\textbf{TODO: meshes, materials, textures, shaders, fat rays, graphics computation machinery is shared between baseline and flexrender}

\subsection{Workers and the Renderer}
\label{workers}

\textbf{TODO: network architecture}

\section{Configuration and Setup}
\label{config}

\textbf{TODO}

\subsection{Synchronization and Asset Distribution}
\label{config}

\textbf{TODO: basic config data, establish worker connections, chunk up morton address space, read meshes, compute morton codes of their centroids, distribute to workers (also mention reading data off of shared network storage)}

\subsection{Parallel Construction of Spatial Acceleration Structures}
\label{parallbvh}

\textbf{TODO: build BVH per mesh, then BVH of mesh bounds (still local), then send worker bounds to renderer and build BVH of worker bounds, distribute that to everyone}

\section{Ray Processing}
\label{process}

\textbf{TODO: mention the 3 types of rays}

\subsection {Ray Queues}
\label{queues}

\textbf{TODO: prefer rays more likely to terminate}

\subsection{Primary Ray Casting}
\label{primaryrays}

\textbf{TODO: runaway detection and prevention}

\subsection{Distributed BVH Traversal}
\label{traversal}

\textbf{TODO: modifications to stackless method}

\subsection{Illumination and Shading}
\label{shading}

\textbf{TODO: casting light rays backwards (light to object) and pushing them back into the system, checking for hit within epsilon of target}

\section{Render Completion}
\label{completion}

\textbf{TODO}

\subsection{Statistics and Monitoring}
\label{stats}

\textbf{TODO: primary casting progress, rays produced and killed, queue sizes}

\subsection{Image Synthesis}
\label{synthesis}

\textbf{TODO: request worker buffers and merge them into a single buffer linearly}

\chapter{Results}
\label{results}

\textbf{TODO}

\chapter{Future Work}
\label{futurework}

\textbf{TODO}

\section{System Optimizations}
\label{optimizations}

\textbf{TODO: replace morton coding with analysis and baking step, distribute all assets except geometry (eliminate final shading hop), prepass step to determine which workers should cast which primary rays}

\section{Memory Optimizations}
\label{memory}

\textbf{TODO: ray pools, cache line size, and queue alignment}

\section{GPGPU and Heterogenous Architectures}
\label{hetergenous}

\textbf{TODO: network interface, GPU workers, 10 GigE user mode DMA}

\clearpage
\bibliography{flexrender}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\section*{Image Results}

\end{document} much faster 
