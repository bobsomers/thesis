
% Cal Poly Thesis
% 
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%




\documentclass[12pt]{ucthesis}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%    \pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue \fi

\usepackage{textcomp}
\usepackage{url}
\usepackage{listings}
\lstset{
	language=[Visual]C++,
	keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
	identifierstyle=\ttfamily,
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
	showstringspaces=false,
	basicstyle=\small,
	numberstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	numbersep=10pt,
	tabsize=2,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	breakatwhitespace=false,
	aboveskip={1.5\baselineskip},
  columns=fixed,
  upquote=true,
  extendedchars=true
% frame=single,
% backgroundcolor=\color{lbcolor},
}
\usepackage{color}
%\ifpdf

    \usepackage[pdftex]{graphicx}
    % Update title and author below...
    \usepackage[pdftex,plainpages=false,breaklinks=true,colorlinks=true,urlcolor=blue,citecolor=blue,%
                                       linkcolor=blue,bookmarks=true,bookmarksopen=true,%
                                       bookmarksopenlevel=3,pdfstartview=FitV,
                                       pdfauthor=Christopher Gibson,
                                       pdftitle=Point-Based Color Bleeding With Volumes,
                                       pdfkeywords={thesis, masters, cal poly, volume rendering, global illumination}
                                       ]{hyperref}
    %Options with pdfstartview are FitV, FitB and FitH
    \pdfcompresslevel=1

%\else
%    \usepackage{graphicx}
%\fi

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}



%%%%%\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}

\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}

\setcounter{tocdepth}{2}


% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

\begin{document}

% Declarations for Front Matter

% Update fields below!
\title{FlexRender: A distributed rendering architecture for ray tracing huge
scenes on commodity hardware.}
\author{Robert Edward Somers}
\degreemonth{June} \degreeyear{2012} \degree{Master of Science}
\defensemonth{June} \defenseyear{2012}
\numberofmembers{3} \chair{Zo\"{e} Wood, Ph.D.} \othermemberA{Chris Lupo, Ph.D.} \othermemberB{Phillip Nico, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}



\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

As the quest for more realistic computer graphics marches steadily on, the
demand for rich and detailed imagery is greater than ever. Unfortunately, our
appetite for large and complex geometry is quickly outpacing advances in the
hardware used to render it. Scenes with hundreds of millions or even billions
of polygons are not only desired, they are demanded.

Techniques such as normal mapping and level of detail have attempted to address
the problem by reducing the amount of geometry in a scene. This is problematic
for applications that desire or demand access to the scene's full geometric
complexity at render time. More recently, out-of-core techniques have provided
methods for rendering large scenes when the working set is larger than the
available system memory.

We propose a distributed rendering architecture based on message-passing that
is designed to partition scene geometry across a cluster of commodity machines
in a spatially coherent way, allowing the entire scene to remain in-core and
enabling the construction of hierarchical spatial acceleration structures in
parallel. The results of our implementation show over an order of magnitude
speedup in rendering time compared the traditional approach, while keeping
memory overhead for message queuing around 1\%.

\textbf{TODO: Triple check that claim with the Toy Store scene results.}

\end{abstract}

%\begin{acknowledgements}

%   Thank you...

%\end{acknowledgements}


\tableofcontents


\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}




\renewcommand{\baselinestretch}{1.66}


% ------------- Main chapters here --------------------





\chapter{Introduction}
\label{intro}

\emph{Rendering}, the process of taking the description of a scene and turning
it into a visual image, has advanced at an incredible pace in recent
years. Many techniques have been developed to turn these descriptions into
pixels, and countless more have targeted weaknesses or challenges with those
algorithms. As the field continues to evolve and grow, new algorithms will be
born, others will fall out of favor and die, and some will soldier on. Only
one thing remains constant: The desire to bring our computational models ever
closer to mimicking physical reality.

\section{Geometric Complexity}
\label{complexity}

At the heart of mimicking reality is describing the world we wish to show. In
computer graphics, this has traditionally been done by defining surfaces. While
exciting developments in volume rendering techniques happen on regular basis,
it's unlikely we'll abandon using surfaces any time soon. Unfortunately for
volumes, they are inherently an $n^3$ problem and $n^3$ is not a particularly
friendly number in the field of Computer Science.

For now and the foreseeable future, it seems, surfaces will be our bread and
butter. Many methods have been developed for describing surfaces to computer
programs, such as level sets, implicitly defined surfaces from mathematical
equations, and curvalinear forms such as parametric splines. The undisputed
champion of surface representations, however, has been the polygonal mesh.
Countless man-years of research and development has yielded efficient
techniques for processing meshes of interconnected polygons at blistering
speeds, and many other surface representations are ultimately converted to a
polygonal mesh at some stage of the rendering pipeline.

Meshes are easy for artists to work with because they represent discrete
points in space and the connectivity between those points (rather than abstract
equations). However, their core advantage is also their core drawback. Because
everything is defined explicitly, meshes with fine levels of detail have
significantly higher storage requirements. Thus, as the demand for higher
visual fidelity increases, the natural tendency is to increase geometric
complexity.

\section{Parallel Rendering}
\label{parallel}

Graphics has long been said to be a problem that is \emph{embarrasingly parallel},
given that many graphics algorithms operate on pixels independently. Graphics
processing units (GPUs) have exploited this fact for many years to achieve
amazing throughput of graphics primitives in real-time. Ray tracing in
particular (discussed more thoroughly in \ref{raytracing}) is commonly
paraded as the poster child for parallel processing.

Unfortunately the story isn't all unicorns and rainbows. While processor
architectures have become exceedingly parallel and posted impressive
improvements, the memory hierarchy hasn't had time to catch up. In fact,
if anything the problem has gotten more muddled and complicated as architects
have scrambled to dump layer upon layer of cache in front of the memory
controller to try to hide the fact that keeping these chips hot all the time is
actually an extremely challenging problem.

\textbf{TODO: graph of latencies in the memory hierarchy}

Because of this, there is a lot more to parallel rendering than initially meets
the eye. Graphics may indeed by highly parallel, but it is also hugely memory
intensive, and that fact is actively working \emph{against} its parallel
nature on current architectures.

\section{Our Contribution}
\label{contribution}

This paper presents the architecture of FlexRender, a ray tracer designed for
rendering huge scenes with high geometric complexity on commodity hardware. We
specifically target commodity hardware because it currently has an excellent
cost to performance ratio, but still typically lacks enough memory to fit large
scenes entirely in RAM.

Current strategies for parallelizing a render across a cluster of commodity
machines are limited to having each worker compute a separate ``slice" of the
image, but do nothing to manage the high cost associated with large scene
assets.

Thus, our work describes the following core contributions:

\begin{enumerate}
    \item A system for ray tracing which uses the pooled memory of a cluster of
        commodity machines to keep the entire scene in-core.
    \item A method for passing ray messages between workers in the cluster with
        enough state to never require a reply message.
    \item An extension to the stackless BVH traversal algorithm presented by
        Hapala et al. \cite{hapala:2011} that makes it possible to suspend
        traversal at one worker and resume it at another.
    \item A discussion of the concepts involved and an analysis of the resulting
        implementation.
\end{enumerate}

In particular, we show that FlexRender can achieve speedups that exceed an
order of magnitude over the traditional parallelization approach, and can
naturally self-regulate the cluster of workers to keep the memory overhead due
to message queueing around 1\% of each worker's system memory.

\textbf{TODO: Triple check that claim with Toy Store results.}

\chapter{Background}
\label{background}

The FlexRender architecture builds on the four fundamenal building blocks
described here. First, we discuss the linearity of light (\ref{radiometry}),
which is critical to understanding why FlexRender produces a correct composite
image from pieces rendered by different machines. Next, we give a general
overview of ray tracing (\ref{raytracing}) and bounding volume hierarchies
(\ref{bvhs}). Finally, we discuss Morton coding and the Z-order curve
(\ref{morton}) which FlexRender uses to distribute scene data amongst the
machines.

\section{Light and Radiometry}
\label{radiometry}

\emph{Radiometry}, the study of propagation of electromagnetic radiation, forms
the basis for many rendering algorithms. Our coverage will be quick and
incomplete, but we'll examine some critical theory that enables the design of
FlexRender. For a more complete treatment of the topic in the context of
rendering algorithms, we refer the reader to
\emph{Physically Based Rendering} \cite{pbrt}.

At its core, radiometry is based on modeling light as radiant energy and
operates at the level of geometric optics. In other words, we acknowledge that
light has wave-like properties and visible light occurs within a spectrum of
wavelengths (roughly 400 to 750 nm), but we do not mathematically model it as
a wave. Rather, we model it as a particle of radiant energy. In ray tracers
(discussed in depth in \ref{raytracing}) we model the path traveled by a single
particle of light with a ray.

The particularly interesting parts of this model with respect to FlexRender are
the following assumptions:

\begin{description}
    \item[Light behaves linearly.] The combined effect of two rays of light
        in a scene is the same as the sum of their independent effects on the
        scene.
    \item[Energy is conserved.] When a ray reflects off of a surface, it can
        never do so with more energy than it started with.
\end{description}

In particular, these assumptions allow us to make the following key
observations, which FlexRender explicitly exploits:

\begin{description}
    \item[The location of computation doesn't matter.] If the scene is
        distributed across many machines, it makes no difference which machine
        computes the effect of a ray. The sum of all the machines' computations
        will be the same as if the all the work was performed on a single
        machine.
    \item[Transmittance models energy conservation.] If we store the amount of
        energy traveling along a ray (the \emph{transmittance}) with the ray
        itself, we need not know anything about the preceeding rays or state
        that brought this ray into existence. We can compute its contribution
        to the scene independently and ensure that linearity and energy
        conservation are both respected.
\end{description}

\section{Ray Tracing}
\label{raytracing}

\textbf{TODO: general overview}

\section{Bounding Volume Hierarchies}
\label{bvhs}

Bounding volume hierarchies, or BVHs, are essentially an application of binary
search to 3D space. In ray tracing, the vast majority of time is spent
computing intersection tests to determine whether a ray intersects a given
primitive. BVHs allow us to search 3D space for a potential intersection in a
binary tree fashion, pruning off large numbers of primitives with a single
test. Since it is effectively binary search, it's unsurprising that it reduces
intersection searching from linear time $O(n)$ to logarithmic time $O(log\;n)$.

\textbf{TODO: picture of bvh}

BVHs are trees where each node is defined by a bounding box that describes
a region of 3D space. All of the primitives in the scene that are within that
region of space are child nodes in the tree. Each node has two direct children,
which also define bounding boxes within that subregion of space. Nodes which
have other bounding box nodes as their children are \emph{interior} nodes.
\emph{Leaf} nodes in the tree define a bounding box around a single primitive.

For storage efficiency and better cache performance, after the tree structure
is built using a typical linked-memory data structure, it's flattened into a
linear array of nodes by walking the tree in a depth-first traversal. This means
that the left-hand child of the node at index $n$ is located at index $n + 1$
in the array of nodes. Each node stores an offset index to its right-hand child.
Because the indices are relative to the array and not the array's location in
memory, they are safe to pass back and forth between machines and require no
address translation.

To traverse the tree looking for intersections, we begin by testing the root
node's bounding box for an intersection. If the ray intersects the bounding
box, we recursively test each of the node's children. Once we hit a leaf node,
we perform an actual ray-primitive intersection test to determine if the ray
hits the primitive.

Because this algorithm is naturally recursive, refactoring it as an iterative
algorithm still requires a traversal stack of child nodes that we need visit on
our way back up. Since this stack is unique for
each ray, the entire stack would need to be carried along with each ray in the
FlexRender design (described in detail in \ref{process}). In addition, the
stack could potentially be large if the scene is large and the tree is deep, so
it would be ideal if we had a method for traversing the tree efficiently without
the need to maintain a traversal stack.

A method for doing so is described by Hapala et al. in
\emph{Efficient Stack-less BVH Traversal for Ray Tracing} \cite{hapala:2011}.
Their key insight is that if parent links are stored in the tree, the same
traversal can be achieved using a three-state automaton describing the direction
you came from when you reached the current node (i.e. from the \emph{parent},
from the \emph{sibling}, or from the \emph{child}). They show that their
traversal algorithm produces identical tree walks and never retests nodes that
have already been tested.

FlexRender leverages this traversal algorithm due it its low state storage
requirements. Each ray only needs to know the index of the current node it's
traversing and the state of the traversal automaton. The extensions made
to the algorithm to support suspending traversal and resuming it on another
machine are described in \ref{traversal}.

\section{Morton Coding and the Z-Order Curve}
\label{morton}

Morton coding is a method for mapping coordinates in multidimensional space to
a single dimension. In particular, walking the multidimensional space with a
Morton coding produces a space-filling Z-order curve.

\textbf{TODO: pictures of Z-order curves}

More concretely, FlexRender needs a way to distribute a large scene to many
machines in a spatially coherent way. If the geometry on each machine consists
of a localized patch of the overall geometry, it allows us minimize communication
between the machines, and thus, only pay the network cost when we absolutely need
to (described in depth in \ref{traversal}).

Because the Morton coding produces a spatially coherent traversal of 3D space,
dividing up the 1D Morton code address space among all the machines participating
in the render gives a reasonable assurance of spatial locality for the geometry
sent to each machine.

The Morton coding is relatively simple to implement. For example, say that
we wish to map a point $P$ in a region of 3D space (defined by its bounding extents
$min$ and $max$) to a Morton coded 64-bit integer. Discretizing each axis evenly
allows for 21 bits per axis, yielding a 63-bit address space (and one unused bit
in the integer).

Computing the Morton code is simply a matter calculating the 21-bit discretized
component of $P$ along each axis, then shifting the components from each axis
into the 64-bit integer one bit at a time in a round robin fashion, from the
most-significant to least-significant bit.

\textbf{TODO: pseudocode listing of morton code computation}

\chapter{Related Work}
\label{relatedwork}

\textbf{TODO}

\section{Big Geometry Workarounds}
\label{managingcomplexity}

\textbf{TODO}

\subsection{Normal Mapping}
\label{normalmaps}

\textbf{TODO: seminal work}

\subsection{Level of Detail}
\label{levelofdetail}

\textbf{TODO: seminal work}

\subsection{Out-of-Core}
\label{outofcore}

\textbf{TODO: dreamworks PBGI paper, panta ray for directional occlusion}

\section{Parallel Rendering}
\label{parallelbg}

\textbf{TODO}

\subsection{Cooperative Networked Rendering}
\label{networked}

\textbf{TODO: how we differ from kilauea (refer to subsections) \cite{kato:2002}}

\subsection{General Purpose GPU Rendering}
\label{gpgpu}

\textbf{TODO: ray batching, parallel BVH construction, branchy/memory drawbacks (shaders)}

\chapter{FlexRender Architecture}
\label{architecture}

\textbf{TODO}

\section{Organization and Design}
\label{organization}

\textbf{TODO}

\subsection{General Types and Subsystems}
\label{types}

\textbf{TODO: meshes, materials, textures, shaders, fat rays, graphics computation machinery is shared between baseline and flexrender}

\subsection{Workers and the Renderer}
\label{workers}

\textbf{TODO: network architecture}

\section{Configuration and Setup}
\label{config}

\textbf{TODO}

\subsection{Synchronization and Asset Distribution}
\label{config}

\textbf{TODO: basic config data, establish worker connections, chunk up morton address space, read meshes, compute morton codes of their centroids, distribute to workers (also mention reading data off of shared network storage)}

\subsection{Parallel Construction of Spatial Acceleration Structures}
\label{parallbvh}

\textbf{TODO: build BVH per mesh, then BVH of mesh bounds (still local), then send worker bounds to renderer and build BVH of worker bounds, distribute that to everyone}

\section{Ray Processing}
\label{process}

\textbf{TODO: mention the 3 types of rays}

\subsection {Ray Queues}
\label{queues}

\textbf{TODO: prefer rays more likely to terminate}

\subsection{Primary Ray Casting}
\label{primaryrays}

\textbf{TODO: runaway detection and prevention}

\subsection{Distributed BVH Traversal}
\label{traversal}

\textbf{TODO: modifications to stackless method}

\subsection{Illumination and Shading}
\label{shading}

\textbf{TODO: casting light rays backwards (light to object) and pushing them back into the system, checking for hit within epsilon of target}

\section{Render Completion}
\label{completion}

\textbf{TODO}

\subsection{Statistics and Monitoring}
\label{stats}

\textbf{TODO: primary casting progress, rays produced and killed, queue sizes}

\subsection{Image Synthesis}
\label{synthesis}

\textbf{TODO: request worker buffers and merge them into a single buffer linearly}

\chapter{Results}
\label{results}

\textbf{TODO}

\chapter{Future Work}
\label{futurework}

\textbf{TODO}

\section{System Optimizations}
\label{optimizations}

\textbf{TODO: replace morton coding with analysis and baking step, distribute all assets except geometry (eliminate final shading hop), prepass step to determine which workers should cast which primary rays}

\section{Memory Optimizations}
\label{memory}

\textbf{TODO: ray pools, cache line size, and queue alignment}

\section{GPGPU and Heterogenous Architectures}
\label{hetergenous}

\textbf{TODO: network interface, GPU workers, 10 GigE user mode DMA}

\clearpage
\bibliography{flexrender}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\section*{Image Results}

\end{document} much faster 
